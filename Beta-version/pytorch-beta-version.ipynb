{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Jagan/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sz = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images; make sure the directory \n",
    "                only contains images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self._files = [x for x in os.listdir(self.root_dir) if x.find('.ipynb') == -1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._files) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self._files[idx])\n",
    "        sample = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample[0,:,:], sample[1:,:,:]/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rgb2lab(image):\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = np.asarray(image)\n",
    "    image = rgb2lab(image/255)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Lambda\n",
    "from torchvision.transforms import RandomAffine\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shear, zoom, rotation and horizontal flip\n",
    "transform = transforms.Compose([RandomAffine(degrees=0.2, shear=0.2),\n",
    "                                RandomHorizontalFlip(p=0.5),\n",
    "                                Resize((256, 256)),\n",
    "                                Lambda(lambda image: transform_rgb2lab(image)),\n",
    "                                transforms.ToTensor(),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ColorizeDataset('../Full-version/Train/', transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Resolve error `num_workers` > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=b_sz, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNetBeta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNetBeta, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, (3,3), stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3,3), stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3,3), stride=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, (3,3), stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3,3), stride=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, (3,3), stride=2)\n",
    "        self.conv7 = nn.Conv2d(256, 512, (3,3), stride=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, (3,3), stride=1)\n",
    "        self.conv9 = nn.Conv2d(256, 128, (3,3), stride=1)\n",
    "        self.upsample10 = nn.Upsample(scale_factor=(2,2))\n",
    "        self.conv11 = nn.Conv2d(128, 64, (3,3), stride=1)\n",
    "        self.upsample12 = nn.Upsample(scale_factor=(2,2))\n",
    "        self.conv13 = nn.Conv2d(64, 32, (3,3), stride=1)\n",
    "        self.conv14 = nn.Conv2d(32, 2, (3,3), stride=1)\n",
    "        self.upsample15 = nn.Upsample(scale_factor=(2,2))\n",
    "   \n",
    "    def same_pad(self, input, k=(3,3), d=(1,1), s=(1,1)):\n",
    "        \n",
    "        i = (input.size(-2), input.size(-1))\n",
    "        # i = (i_H, i_W)\n",
    "        # k = (k_H, k_W)\n",
    "        # d = (d, d); dilation\n",
    "        # s = (s, s); stride\n",
    "        \n",
    "        # tensorflow style - same padding output calculation\n",
    "        calc_eff_k = lambda k, d: (k - 1) * d + 1\n",
    "        k = tuple([calc_eff_k(x, y) for x, y in zip(k, d)])\n",
    "        calc_o = lambda i, s: np.ceil(i/s)\n",
    "        o = tuple([calc_o(x, y) for x, y in zip(i, s)])\n",
    "        calc_p = lambda o, s, k, i: max(0, (o-1)*s + k-i)\n",
    "        p = tuple([calc_p(w, x, y, z) for w, x, y, z in zip(o, s, k, i)])\n",
    "                               \n",
    "        # left, right , up , bottom\n",
    "        padding = [p[1]//2, p[1]//2 + p[1]%2, p[0]//2, p[0]//2 + p[0]%2]\n",
    "        padding = [int(p) for p in padding]\n",
    "        return F.pad(input, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(self.same_pad(x)))\n",
    "        x = F.relu(self.conv2(self.same_pad(x, s=(2, 2))))\n",
    "        x = F.relu(self.conv3(self.same_pad(x)))\n",
    "        x = F.relu(self.conv4(self.same_pad(x, s=(2, 2))))\n",
    "        x = F.relu(self.conv5(self.same_pad(x)))\n",
    "        x = F.relu(self.conv6(self.same_pad(x, s=(2, 2))))\n",
    "        x = F.relu(self.conv7(self.same_pad(x)))\n",
    "        x = F.relu(self.conv8(self.same_pad(x)))\n",
    "        x = F.relu(self.conv9(self.same_pad(x)))\n",
    "        x = self.upsample10(x)\n",
    "        x = F.relu(self.conv11(self.same_pad(x)))\n",
    "        x = self.upsample12(x)\n",
    "        x = F.relu(self.conv13(self.same_pad(x)))\n",
    "        x = F.tanh(self.conv14(self.same_pad(x)))\n",
    "        x = self.upsample15(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ColorNetBeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=net.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/Jagan/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/Jagan/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "  5%|▌         | 10/200 [00:07<02:30,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.0025241198018193245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [00:15<02:20,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.001218198100104928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [00:23<02:12,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, loss: 0.001563854282721877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [00:30<02:03,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, loss: 0.0017088993918150663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:38<01:56,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50, loss: 0.0016615402419120073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [00:46<01:48,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60, loss: 0.0017109338659793139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [00:54<01:40,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70, loss: 0.000922243227250874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [01:01<01:32,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80, loss: 0.0017484532436355948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [01:09<01:24,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90, loss: 0.0010772462701424956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [01:17<01:17,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss: 0.0006255232729017735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [01:24<01:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 110, loss: 0.0006199725903570652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120/200 [01:32<01:01,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, loss: 0.0019473774591460824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [01:40<00:53,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 130, loss: 0.001128384843468666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [01:47<00:46,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 140, loss: 0.0008004392730072141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [01:55<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150, loss: 0.0011302819475531578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [02:03<00:30,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 160, loss: 0.0005982593866065145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170/200 [02:11<00:23,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 170, loss: 0.000842070730868727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [02:18<00:15,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 180, loss: 0.0003926165518350899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190/200 [02:26<00:07,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190, loss: 0.00070851860800758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:34<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, loss: 0.0004900220083072782\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(200)):  \n",
    "    running_loss = 0.0\n",
    "    # get the inputs\n",
    "    for X, Y in dataloader:\n",
    "        bz = X.shape[0]\n",
    "        X, Y = X.view(bz, -1, 256, 256), Y.view(bz, -1, 256, 256)\n",
    "        inputs, labels = X.float().to(device), Y.float().to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if epoch % 10 == 9:    # print every 10 epochs\n",
    "        print(f'epoch: {epoch + 1}, loss: {running_loss / 4}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images; make sure the directory \n",
    "                only contains images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self._files = [x for x in os.listdir(self.root_dir) if x.find('.ipynb') == -1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._files) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self._files[idx])\n",
    "        sample = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return img_name, sample[0,:,:], sample[1:,:,:]/128 # added file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ColorizeDataset('../Full-version/Test/', transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './result_pytorch'\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Jagan/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/Jagan/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/Jagan/anaconda3/envs/fastai/lib/python3.6/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0018670042045414448\n",
      "loss: 0.0025375480763614178\n",
      "loss: 0.006047520786523819\n",
      "loss: 0.0003591009881347418\n",
      "loss: 0.016005825251340866\n",
      "loss: 0.002735728397965431\n",
      "loss: 0.005061606410890818\n",
      "loss: 0.008008704520761967\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for img_name, X, Y in test_dataloader:\n",
    "        bz = X.shape[0] # bz should be 1\n",
    "        X, Y = X.view(bz, -1, 256, 256), Y.view(bz, -1, 256, 256)\n",
    "        inputs, labels = X.float().to(device), Y.float().to(device)\n",
    "        # inference\n",
    "        preds = net(inputs)\n",
    "        loss = criterion(preds, labels)\n",
    "        print(f'loss: {loss}')\n",
    "        ab = np.transpose(preds.cpu().numpy(), (0, 2, 3, 1)) * 128\n",
    "        L = np.transpose(inputs.cpu().numpy(), (0, 2, 3, 1))\n",
    "\n",
    "        output = np.empty(shape=(256, 256, 3))\n",
    "        output[:,:,0] = np.squeeze(L)\n",
    "        output[:,:,1:] = np.squeeze(ab)\n",
    "        imsave(f\"{os.path.join(save_dir, os.path.basename(img_name[0]))[:-4]}.png\", lab2rgb(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
